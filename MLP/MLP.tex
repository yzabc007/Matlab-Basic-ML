\documentclass[UTF-8]{article}
\title{MLP}
\author{Wayne Wang}
\date{}

\usepackage{tikz}
\usetikzlibrary{arrows,snakes,backgrounds}
\usepackage{amsmath}

\begin{document}
\maketitle
\section{Principles of Backpropagation}

\tikzstyle{place}=[circle,draw=blue!50,fill=blue!20,thick,
inner sep=0pt,minimum size=6mm]
\begin{center}
  \begin{tikzpicture}[node distance=1.5cm]
    \node[place] (x0) {\textit{+1}};
    \node[place] (x1) [below of=x0, label=above:$x_i$] {};
    \node[place] (x2) [below of=x1] {};
    \node[place] (x3) [below of=x2] {};
    \node[place] (h0) [right of=x0] {\textit{+1}};
    \node[place] (h1) [below of=h0, label=above:$h_j\to a_j$] {}
      edge [<-] (x0)
      edge [<-] node[auto,swap] {$v_{ij}$} (x1)
      edge [<-] (x2)
      edge [<-] (x3);
    \node[place] (h2) [below of=h1] {}
      edge [<-] (x0)
      edge [<-] (x1)
      edge [<-] (x2)
      edge [<-] (x3);
    \node[place] (h3) [below of=h2] {}
      edge [<-] (x0)
      edge [<-] (x1)
      edge [<-] (x2)
      edge [<-] (x3);
    \node[place] (y1) [right of=h2, label=above:$b_k\to y_k$] {}
      edge [<-] (h0)
      edge [<-] node[auto,swap] {$w_{jk}$} (h1)
      edge [<-] (h2)
      edge [<-] (h3);
    \node[place] (z1) [right of=y1, label=above:$t_k$] {};
  \end{tikzpicture}
\end{center}

Suppost we have a simple binary classification problem that
needs to be solved by MLP. The structure of the MLP is shown as
above which has 3 input nodes for input layer, 3 hidden nodes
for one hidden layer and 1 output node for output layer.

The activation function is sigmoid function:
$g(x)=\frac{1}{1+e^{-x}}$ and $g'(x)=g(x)(1-g(x))$.
\\

Step 1: \textbf{Going forward}
\begin{gather}
  h_j = \sum_j x_i\cdot v_{ij}\\
  a_j = g(h_j)\\
  b_k = \sum_k a_j\cdot w_{jk}\\
  y_k = g(b_k)
\end{gather}

Step 2: \textbf{Compute Error}
\begin{equation}
  \begin{split}
  E(\textbf{v},\textbf{w})
  &=\frac{1}{2} \sum_k (t_k-y_k)^2\\
  &=\frac{1}{2} \sum_k (t_k-g(b_k)^2\\
  &=\frac{1}{2} \sum_k (t_k-g(\sum_j a_j\cdot w_{jk}))^2\\
  &=\frac{1}{2} \sum_k (t_k-g(\sum_j g(h_j)\cdot w_{jk})^2\\
  &=\frac{1}{2} \sum_k (t_k-g(\sum_j g(\sum_i x_i\cdot v_{ij})\cdot w_{jk})^2\\
\end{split}
\end{equation}

Step 3: Compute the derivative of Error function
respect to \textbf{w} and \textbf{v}

\begin{equation}
  \begin{split}
    \frac{\partial E}{\partial w_{jk}}
    &=\frac{\partial}{\partial w_{jk}} (\frac{1}{2} \sum_k (t_k-y_k)^2)\\
    &=\sum_k (t_k-y_k)\cdot \frac{\partial}{\partial w_{jk}} (t_k-y_k)\\
    &=\sum_k (t_k-y_k)\cdot \frac{\partial}{\partial w_{jk}} (-g(b_k))\\
    &=\sum_k (t_k-y_k)\cdot -g'(b_k)\cdot \frac{\partial}{\partial w_{jk}} (\sum_j a_j\cdot w_{jk})\\
    &=\sum_k (y_k-t_k)\cdot y_k\cdot(1-y_k)\cdot a_j
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \frac{\partial E}{\partial v_{ij}}
    &=\frac{\partial E}{\partial a_j}\cdot \frac{\partial a_j}{\partial v_{ij}}\\
    &=\frac{\partial}{\partial a_j} (\frac{1}{2} \sum_k (t_k-g(\sum_j a_j\cdot w_{jk}))^2) \cdot \frac{\partial}{\partial v_{ij}} (g(\sum_i x_i\cdot v_{ij}))\\
    &=(\sum_k (y_k-t_k)\cdot y_k \cdot (1-y_k)\cdot w_{jk}) \cdot a_j\cdot (1-a_j)\cdot x_i
  \end{split}
\end{equation}

Step 4: Update weight \textbf{w} and \textbf{v}
\begin{gather}
  w_jk = w_jk - \eta \frac{\partial E}{\partial w_{jk}}\\
  v_ij = v_ij - \eta \frac{\partial E}{\partial v_{ij}}
\end{gather}

\end{document}
